{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, RobustScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import os\n",
    "import nltk\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_prep(name):\n",
    "    df = pd.read_json(os.path.join('{}.json'.format(name)))\n",
    "    df.set_index('id', inplace=True)\n",
    "    df['ingredients_count'] = df['ingredients'].apply(lambda x: len(x))\n",
    "    df['ingredients_word_count'] = df['ingredients'].apply(lambda ingredients: [len(i.split()) for i in ingredients])\n",
    "    df['ingredients'] = df['ingredients'].astype(str).apply(lambda ingredients: re.sub('\\[|\\]', '', ingredients)) #ingredients of two words can get lost\n",
    "    #     df['ingredients'] = df['ingredients'].apply(lambda ingredients: ' '.join(ingredients)) #ingredients of two words can get lost\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_count</th>\n",
       "      <th>ingredients_word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10259</th>\n",
       "      <td>greek</td>\n",
       "      <td>'romaine lettuce', 'black olives', 'grape toma...</td>\n",
       "      <td>9</td>\n",
       "      <td>[2, 2, 2, 1, 1, 2, 1, 2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25693</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>'plain flour', 'ground pepper', 'salt', 'tomat...</td>\n",
       "      <td>11</td>\n",
       "      <td>[2, 2, 1, 1, 3, 1, 1, 2, 3, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20130</th>\n",
       "      <td>filipino</td>\n",
       "      <td>'eggs', 'pepper', 'salt', 'mayonaise', 'cookin...</td>\n",
       "      <td>12</td>\n",
       "      <td>[1, 1, 1, 1, 2, 2, 3, 2, 2, 2, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22213</th>\n",
       "      <td>indian</td>\n",
       "      <td>'water', 'vegetable oil', 'wheat', 'salt'</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13162</th>\n",
       "      <td>indian</td>\n",
       "      <td>'black pepper', 'shallots', 'cornflour', 'caye...</td>\n",
       "      <td>20</td>\n",
       "      <td>[2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cuisine                                        ingredients  \\\n",
       "id                                                                      \n",
       "10259        greek  'romaine lettuce', 'black olives', 'grape toma...   \n",
       "25693  southern_us  'plain flour', 'ground pepper', 'salt', 'tomat...   \n",
       "20130     filipino  'eggs', 'pepper', 'salt', 'mayonaise', 'cookin...   \n",
       "22213       indian          'water', 'vegetable oil', 'wheat', 'salt'   \n",
       "13162       indian  'black pepper', 'shallots', 'cornflour', 'caye...   \n",
       "\n",
       "       ingredients_count                             ingredients_word_count  \n",
       "id                                                                           \n",
       "10259                  9                        [2, 2, 2, 1, 1, 2, 1, 2, 3]  \n",
       "25693                 11                  [2, 2, 1, 1, 3, 1, 1, 2, 3, 1, 2]  \n",
       "20130                 12               [1, 1, 1, 1, 2, 2, 3, 2, 2, 2, 1, 2]  \n",
       "22213                  4                                       [1, 2, 1, 1]  \n",
       "13162                 20  [2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = load_n_prep('train')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join('train.json'))\n",
    "total = []\n",
    "for i, ind in df.ingredients.iteritems():\n",
    "    total += ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    oz tomato sauce',\n",
       " '   oz tomato paste',\n",
       " '1 lowfat buttermilk',\n",
       " '1 lowfat chocolate milk',\n",
       " '1 lowfat cottage cheese',\n",
       " '1 lowfat milk',\n",
       " '10 oz frozen chopped spinach',\n",
       " '10 oz frozen chopped spinach thawed and squeezed dry',\n",
       " '14 oz sweetened condensed milk',\n",
       " '145 oz diced tomatoes',\n",
       " '15 oz refried beans',\n",
       " '2 12 to 3 lb chicken cut into serving pieces',\n",
       " '2 low fat cheddar chees',\n",
       " '2 lowfat cottage cheese',\n",
       " '2 lowfat greek yogurt',\n",
       " '2 milk shredded mozzarella cheese',\n",
       " '2 reducedfat milk',\n",
       " '25 less sodium chicken broth',\n",
       " '33 less sodium cooked deli ham',\n",
       " '33 less sodium cooked ham',\n",
       " '33 less sodium ham',\n",
       " '33 less sodium smoked fully cooked ham',\n",
       " '40 less sodium taco seasoning',\n",
       " '40 less sodium taco seasoning mix',\n",
       " '7 Up',\n",
       " '8 ounc ziti pasta cook and drain',\n",
       " '95 lean ground beef',\n",
       " 'A Taste of Thai Rice Noodles',\n",
       " 'Accent Seasoning',\n",
       " 'Adobo All Purpose Seasoning',\n",
       " 'Alaskan king crab legs',\n",
       " 'Alexia Waffle Fries',\n",
       " 'Alfredo sauce',\n",
       " 'Amarena cherries',\n",
       " 'Amaretti Cookies',\n",
       " 'American cheese',\n",
       " 'Anaheim chile',\n",
       " 'Angostura bitters',\n",
       " 'Argo Corn Starch',\n",
       " 'Asian chili sauce',\n",
       " 'Asian sweet chili sauce',\n",
       " 'Azteca Flour Tortillas',\n",
       " 'BACARDI Mixers Margarita Mix',\n",
       " 'BACARDI Superior',\n",
       " 'BREAKSTONES Sour Cream',\n",
       " 'Baileys Irish Cream Liqueur',\n",
       " 'Balsamico Bianco',\n",
       " 'Barilla Linguine',\n",
       " 'Barilla OvenReady Lasagne',\n",
       " 'Barilla Plus Pasta',\n",
       " 'Bartlett Pear',\n",
       " 'Belgian endive',\n",
       " 'Bengali 5 Spice',\n",
       " 'Bertolli Alfredo Sauce',\n",
       " 'Bertolli Arrabbiata Sauce',\n",
       " 'Bertolli Classico Olive Oil',\n",
       " 'Bertolli Garlic Alfredo Sauce',\n",
       " 'Bertolli Tomato  Basil Sauce',\n",
       " 'Best Foods Mayonnaise with Lime Juice',\n",
       " 'Best Foods Real Mayonnaise',\n",
       " 'Better Than Bouillon Chicken Base',\n",
       " 'Betty Crocker oatmeal cookie mix',\n",
       " 'Biryani Masala',\n",
       " 'Bisquick Baking Mix',\n",
       " 'Bisquick Original AllPurpose Baking Mix',\n",
       " 'Bob Evans Italian Sausage',\n",
       " 'Bordelaise sauce',\n",
       " 'Boston lettuce',\n",
       " 'Boursin',\n",
       " 'Braeburn Apple',\n",
       " 'Bragg Liquid Aminos',\n",
       " 'Breakstones Sour Cream',\n",
       " 'Breyers Natural Vanilla Ice Cream',\n",
       " 'Budweiser',\n",
       " 'Burgundy wine',\n",
       " 'CURRY GUY Smoked Garam Masala',\n",
       " 'CURRY GUY Smoked Spicy Salt',\n",
       " 'California bay leaves',\n",
       " 'Camellia Red Kidney Beans',\n",
       " 'Campbells Condensed Cheddar Cheese Soup',\n",
       " 'Campbells Condensed Cream of Chicken Soup',\n",
       " 'Campbells Condensed Cream of Mushroom Soup',\n",
       " 'Campbells Condensed Tomato Soup',\n",
       " 'Cara Cara orange',\n",
       " 'Castelvetrano olives',\n",
       " 'Cavenders Greek Seasoning',\n",
       " 'Challenge Butter',\n",
       " 'Chambord Liqueur',\n",
       " 'Chartreuse Liqueur',\n",
       " 'Chianti',\n",
       " 'Chinese egg noodles',\n",
       " 'Chinese rice vinegar',\n",
       " 'Chinese rose wine',\n",
       " 'Chinese sesame paste',\n",
       " 'Cholula Hot Sauce',\n",
       " 'Ciabatta rolls',\n",
       " 'Cinnamon Toast Crunch Cereal',\n",
       " 'Classico Pasta Sauce',\n",
       " 'Cointreau Liqueur',\n",
       " 'Colmans Mustard Powder',\n",
       " 'Conimex Wok Olie',\n",
       " 'Conimex Woksaus Specials Vietnamese Gember Knoflook',\n",
       " 'Corn Flakes Cereal',\n",
       " 'Country Crock Spread',\n",
       " 'Coxs Orange Pippin',\n",
       " 'Crisco Pure Canola Oil',\n",
       " 'Crisco Pure Vegetable Oil',\n",
       " 'Crispy Rice Cereal',\n",
       " 'Crystal Farms Butter',\n",
       " 'Crystal Farms Reduced Fat Shredded Marble Jack Cheese',\n",
       " 'Crystal Farms Shredded Cheddar Cheese',\n",
       " 'Crystal Farms Shredded Gouda Cheese',\n",
       " 'Crystal Hot Sauce',\n",
       " 'Daisy Sour Cream',\n",
       " 'Daiya',\n",
       " 'DeLallo Extra Virgin Olive Oil',\n",
       " 'DeLallo Penne Ziti',\n",
       " 'Diamond Crystal Kosher Salt',\n",
       " 'Dole Seven Lettuces',\n",
       " 'Domino Confectioners Sugar',\n",
       " 'Domino Light Brown Sugar',\n",
       " 'Doritos Tortilla Chips',\n",
       " 'Doubanjiang',\n",
       " 'Dungeness crabs',\n",
       " 'Dutchprocessed cocoa powder',\n",
       " 'Earth Balance Buttery Spread',\n",
       " 'Earth Balance Natural Buttery Spread',\n",
       " 'Edam',\n",
       " 'Egglands Best eggs',\n",
       " 'Elmlea Single Light',\n",
       " 'Elmlea single',\n",
       " 'Emmenthal',\n",
       " 'English muffins',\n",
       " 'English mustard',\n",
       " 'English toffee bits',\n",
       " 'Equal Sweetener',\n",
       " 'Estancia Pinot Noir',\n",
       " 'Everglades Seasoning',\n",
       " 'Fisher Pecan Halves',\n",
       " 'Fisher Pecans',\n",
       " 'Flora Buttery',\n",
       " 'Flora Cuisine',\n",
       " 'Flora Original',\n",
       " 'Flora proactiv',\n",
       " 'Foster Farms boneless skinless chicken breasts',\n",
       " 'Frangelico',\n",
       " 'Franks Hot Sauce',\n",
       " 'Franks RedHot Original Cayenne Pepper Sauce',\n",
       " 'French bread loaves',\n",
       " 'French lentils',\n",
       " 'French mustard',\n",
       " 'Fuji Apple',\n",
       " 'Fuyu persimmons',\n",
       " 'Galliano',\n",
       " 'Gebhardt Chili Powder',\n",
       " 'Gochujang base',\n",
       " 'Godiva Chocolate Liqueur',\n",
       " 'Gold Medal All Purpose Flour',\n",
       " 'Gold Medal Flour',\n",
       " 'Good Seasons Italian Dressing Mix',\n",
       " 'Gourmet Garden Oregano',\n",
       " 'Gourmet Garden Parsley',\n",
       " 'Gourmet Garden garlic paste',\n",
       " 'Goya Extra Virgin Olive Oil',\n",
       " 'Goya Ground Cumin',\n",
       " 'Goya Hot Sauce',\n",
       " 'Grand Marnier',\n",
       " 'Greek black olives',\n",
       " 'Greek dressing',\n",
       " 'Greek feta',\n",
       " 'Green Giant Whole Kernel Sweet Corn',\n",
       " 'Green Giant sliced mushrooms',\n",
       " 'Guinness Beer',\n",
       " 'Guinness Lager',\n",
       " 'Haas avocados',\n",
       " 'Hatch Green Chiles',\n",
       " 'Hawaiian salt',\n",
       " 'Heath Candy Bars',\n",
       " 'Heinz Chili Sauce',\n",
       " 'Heinz Ketchup',\n",
       " 'Heinz Tomato Ketchup',\n",
       " 'Heinz Worcestershire Sauce',\n",
       " 'Hellmanns Dijonnaise Creamy Dijon Mustard',\n",
       " 'Hellmanns Light Mayonnaise',\n",
       " 'Hellmanns Real Mayonnaise',\n",
       " 'Herdez Salsa',\n",
       " 'Herdez Salsa Casera',\n",
       " 'Herdez Salsa Verde',\n",
       " 'Hidden Valley Farmhouse Originals Italian with Herbs Dressing',\n",
       " 'Hidden Valley Greek Yogurt Original Ranch Dip Mix',\n",
       " 'Hidden Valley Original Ranch Dips Mix',\n",
       " 'Hidden Valley Original Ranch Dressing',\n",
       " 'Hidden Valley Original Ranch Light Dressing',\n",
       " 'Hidden Valley Original Ranch Salad Dressing  Seasoning Mix',\n",
       " 'Hidden Valley Original Ranch Spicy Ranch Dressing',\n",
       " 'Himalayan salt',\n",
       " 'Hogue Cabernet Sauvignon',\n",
       " 'Holland House White Wine Vinegar',\n",
       " 'Homemade Yogurt',\n",
       " 'Honeysuckle White Hot Italian Turkey Sausage Links',\n",
       " 'Hurst Family Harvest Chipotle Lime Black Bean Soup mix',\n",
       " 'I Cant Believe Its Not Butter All Purpose Sticks',\n",
       " 'I Cant Believe Its Not Butter Spread',\n",
       " 'Ibarra Chocolate',\n",
       " 'Imperial Sugar Light Brown Sugar',\n",
       " 'India Pale Ale',\n",
       " 'Indian spice',\n",
       " 'Irish Red ale',\n",
       " 'Irish whiskey',\n",
       " 'Italian basil',\n",
       " 'Italian bread',\n",
       " 'Italian cheese',\n",
       " 'Italian cheese blend',\n",
       " 'Italian herbs',\n",
       " 'Italian parsley leaves',\n",
       " 'Italian seasoned breadcrumbs',\n",
       " 'Italian seasoned diced tomatoes',\n",
       " 'Italian seasoned panko bread crumbs',\n",
       " 'Italian turkey sausage',\n",
       " 'Italian turkey sausage links',\n",
       " 'JOHNSONVILLE Hot  Spicy Sausage Slices',\n",
       " 'JOHNSONVILLE Hot N Spicy Brats',\n",
       " 'Jack Daniels Whiskey',\n",
       " 'Jagermeister Liqueur',\n",
       " 'Jamaican allspice',\n",
       " 'Jameson Irish Whiskey',\n",
       " 'Jameson Whiskey',\n",
       " 'Japanese Mayonnaise',\n",
       " 'Japanese mountain yam',\n",
       " 'Japanese rice vinegar',\n",
       " 'Japanese soy sauce',\n",
       " 'Japanese turnips',\n",
       " 'Jarlsberg',\n",
       " 'Jasmine brown rice',\n",
       " 'JellO Gelatin',\n",
       " 'JellO Gelatin Dessert',\n",
       " 'Jif Creamy Peanut Butter',\n",
       " 'Jiffy Corn Muffin Mix',\n",
       " 'Jimmy Dean All Natural Regular Pork Sausage',\n",
       " 'Jimmy Dean Pork Sausage',\n",
       " 'Johnsonville Andouille',\n",
       " 'Johnsonville Andouille Dinner Sausage',\n",
       " 'Johnsonville Andouille Fully Cooked Sausage',\n",
       " 'Johnsonville Hot  Spicy Breakfast Links',\n",
       " 'Johnsonville Mild Italian Ground Sausage',\n",
       " 'Johnsonville Mild Italian Sausage Links',\n",
       " 'Johnsonville Smoked Sausage',\n",
       " 'Jonshonville Cajun Style Chicken Sausage',\n",
       " 'Jose Cuervo',\n",
       " 'Jose Cuervo Gold Tequila',\n",
       " 'KNUDSEN 2 Milkfat Low Fat Cottage Cheese',\n",
       " 'KRAFT Mexican Style 2 Milk Finely Shredded Four Cheese',\n",
       " 'KRAFT Mexican Style Finely Shredded Four Cheese',\n",
       " 'KRAFT Mexican Style Shredded Four Cheese with a TOUCH OF PHILADELPHIA',\n",
       " 'KRAFT Original Barbecue Sauce',\n",
       " 'KRAFT Reduced Fat Shredded Mozzarella Cheese',\n",
       " 'KRAFT Shredded Cheddar Cheese',\n",
       " 'KRAFT Shredded Colby  Monterey Jack Cheese',\n",
       " 'KRAFT Shredded LowMoisture PartSkim Mozzarella Cheese',\n",
       " 'KRAFT Shredded Mozzarella Cheese',\n",
       " 'KRAFT Shredded Pepper Jack Cheese with a TOUCH OF PHILADELPHIA',\n",
       " 'KRAFT Zesty Italian Dressing',\n",
       " 'KRAFT Zesty Lime Vinaigrette Dressing',\n",
       " 'Kahlua Liqueur',\n",
       " 'Karo Corn Syrup',\n",
       " 'Kerrygold Pure Irish Butter',\n",
       " 'Kewpie Mayonnaise',\n",
       " 'Kikkoman Less Sodium Soy Sauce',\n",
       " 'Kikkoman Oyster Sauce',\n",
       " 'Kikkoman Soy Sauce',\n",
       " 'Kim Crawford Sauvignon Blanc',\n",
       " 'Kitchen Bouquet',\n",
       " 'Klondike Rose red skin potato',\n",
       " 'Knorr Beef Bouillon',\n",
       " 'Knorr Beef Stock Cubes',\n",
       " 'Knorr Chicken Flavor Bouillon',\n",
       " 'Knorr Chicken Stock Cubes',\n",
       " 'Knorr Chicken Stock Pots',\n",
       " 'Knorr Fiesta Sides  Mexican Rice',\n",
       " 'Knorr Fiesta Sides Spanish Rice',\n",
       " 'Knorr Fish Stock Cubes',\n",
       " 'Knorr Onion Minicubes',\n",
       " 'Knorr Pasta Sides  Alfredo',\n",
       " 'Knorr Pasta Sides  Butter  Herb',\n",
       " 'Knorr Pasta Sides  Chicken flavor',\n",
       " 'Knorr Vegetable recipe mix',\n",
       " 'Knox unflavored gelatin',\n",
       " 'Knudsen Light Sour Cream',\n",
       " 'Knudsen Sour Cream',\n",
       " 'Korean chile flakes',\n",
       " 'Kraft Big Slice Pepper Jack Cheese Slices',\n",
       " 'Kraft Extra Sharp Cheddar Cheese',\n",
       " 'Kraft Grated Parmesan Cheese',\n",
       " 'Kraft Mayonnaise',\n",
       " 'Kraft Miracle Whip Dressing',\n",
       " 'Kraft Sharp Cheddar Cheese',\n",
       " 'Kraft Shredded Pepper Jack Cheese',\n",
       " 'Kraft Slim Cut Mozzarella Cheese Slices',\n",
       " 'Kraft Sun Dried Tomato Vinaigrette',\n",
       " 'Kroger Black Beans',\n",
       " 'Kung Pao sauce',\n",
       " 'La Victoria Red Chile Sauce',\n",
       " 'Land O Lakes Butter',\n",
       " 'Lea  Perrins Worcestershire Sauce',\n",
       " 'Lipton Iced Tea Brew Family Size Tea Bags',\n",
       " 'Lipton Lemon Iced Tea Mix',\n",
       " 'Lipton Recipe Secrets Onion Soup Mix',\n",
       " 'Lipton Sparkling Diet Green Tea with Strawberry Kiwi',\n",
       " 'Louisiana Cajun Seasoning',\n",
       " 'Louisiana Hot Sauce',\n",
       " 'MMs Candy',\n",
       " 'Madeira',\n",
       " 'Madras curry powder',\n",
       " 'Mae Ploy Sweet Chili Sauce',\n",
       " 'Maggi',\n",
       " 'Makers Mark Whisky',\n",
       " 'Manischewitz Matzo Meal',\n",
       " 'Manischewitz Potato Starch',\n",
       " 'Manzanilla olives',\n",
       " 'Marcona almonds',\n",
       " 'Margherita Pepperoni',\n",
       " 'Marshmallow Fluff',\n",
       " 'Martha White Cornbread Mix',\n",
       " 'Massaman curry paste',\n",
       " 'Mazola Canola Oil',\n",
       " 'Mazola Chicken Flavor Bouillon Powder',\n",
       " 'Mazola Corn Oil',\n",
       " 'McCormick Ground Cumin',\n",
       " 'McCormick Ground Ginger',\n",
       " 'McCormick Ground White Pepper',\n",
       " 'McCormick Parsley Flakes',\n",
       " 'McCormick Poppy Seed',\n",
       " 'McCormick Pure Vanilla Extract',\n",
       " 'McCormick Taco Seasoning',\n",
       " 'Melba toast',\n",
       " 'Mexican beer',\n",
       " 'Mexican cheese',\n",
       " 'Mexican cheese blend',\n",
       " 'Mexican lager beer',\n",
       " 'Mexican oregano',\n",
       " 'Mexican seasoning mix',\n",
       " 'Mexican vanilla extract',\n",
       " 'Meyer lemon juice',\n",
       " 'Meyer lemon peel',\n",
       " 'Mezzetta Sliced Greek Kalamata Olives',\n",
       " 'Minute White Rice',\n",
       " 'Mission Corn Tortillas',\n",
       " 'Mission Yellow Corn Tortillas',\n",
       " 'Mizkan Oigatsuo Tsuyu Soup Base',\n",
       " 'Mizkan Rice Vinegar',\n",
       " 'Mo Qua',\n",
       " 'Morton Salt',\n",
       " 'Mountain Dew Soda',\n",
       " 'Mrs Dash',\n",
       " 'Nakano Seasoned Rice Vinegar',\n",
       " 'Neapolitan ice cream',\n",
       " 'Nestle Table Cream',\n",
       " 'Neufchâtel',\n",
       " 'New York Style Panetini toasts',\n",
       " 'Nido Milk Powder',\n",
       " 'NielsenMassey Vanilla Extract',\n",
       " 'Nilla Wafers',\n",
       " 'Niçoise olives',\n",
       " 'Nutella',\n",
       " 'OREO Cookies',\n",
       " 'Old Bay Blackened Seasoning',\n",
       " 'Old El Paso Enchilada Sauce',\n",
       " 'Old El Paso Flour Tortillas',\n",
       " 'Old El Paso Green Chiles',\n",
       " 'Old El Paso Taco Seasoning Mix',\n",
       " 'Old El Paso Thick n Chunky salsa',\n",
       " 'Old El Paso chopped green chiles',\n",
       " 'Old El Paso mild red enchilada sauce',\n",
       " 'Old El Paso refried beans',\n",
       " 'Old El Paso taco seasoning mix',\n",
       " 'Ortega Tostada Shells',\n",
       " 'Oscar Mayer Bacon',\n",
       " 'Oscar Mayer Cotto Salami',\n",
       " 'Oscar Mayer Deli Fresh Smoked Ham',\n",
       " 'Pace Chunky Salsa',\n",
       " 'Pace Picante Sauce',\n",
       " 'Pale Ale',\n",
       " 'Pam Cooking Spray',\n",
       " 'Pam NoStick Cooking Spray',\n",
       " 'Pepperidge Farm Puff Pastry Sheets',\n",
       " 'Pernod Liqueur',\n",
       " 'Philadelphia Cooking Creme',\n",
       " 'Philadelphia Cream Cheese',\n",
       " 'Philadelphia Light Cream Cheese',\n",
       " 'Pillsbury Crescent Recipe Creations refrigerated seamless dough sheet',\n",
       " 'Pillsbury Pie Crusts',\n",
       " 'Pillsbury Refrigerated Crescent Dinner Rolls',\n",
       " 'Pillsbury Thin Pizza Crust',\n",
       " 'Pillsbury classic pizza crust',\n",
       " 'Piment dEspelette',\n",
       " 'Poire Williams',\n",
       " 'Pompeian Canola Oil and Extra Virgin Olive Oil',\n",
       " 'Potatoes OBrien',\n",
       " 'Praline Liqueur',\n",
       " 'Progresso Artichoke Hearts',\n",
       " 'Progresso Balsamic Vinegar',\n",
       " 'Progresso Black Beans',\n",
       " 'Progresso Chicken Broth',\n",
       " 'Progresso Diced Tomatoes',\n",
       " 'Pure Wesson Canola Oil',\n",
       " 'Pure Wesson Vegetable Oil',\n",
       " 'Quinoa Flour',\n",
       " 'Quorn Chikn Tenders',\n",
       " 'Quorn crumbles',\n",
       " 'Ragu Classic Alfredo Sauce',\n",
       " 'Ragu Golden Veggie Fettuccine Pasta',\n",
       " 'Ragu Robusto Pasta Sauce',\n",
       " 'Ragu Sauce',\n",
       " 'Ragu Traditional Sauce',\n",
       " 'Ranch Style Beans',\n",
       " 'Reblochon',\n",
       " 'Red Gold diced tomatoes',\n",
       " 'Rice Krispies Cereal',\n",
       " 'Ritz Crackers',\n",
       " 'RoTel Diced Tomatoes  Green Chilies',\n",
       " 'Robert Mondavi Fume Blanc',\n",
       " 'Rotel Diced Tomatoes  Green Chilies',\n",
       " 'Royal Baking Powder',\n",
       " 'SYD Hot Rub',\n",
       " 'Saffron Road Vegetable Broth',\n",
       " 'Saigon cinnamon',\n",
       " 'San Marzano Crushed Tomatoes',\n",
       " 'San Marzano Diced Tomatoes',\n",
       " 'San Marzano tomatoes',\n",
       " 'Sangiovese',\n",
       " 'Sargento Artisan Blends Shredded Parmesan Cheese',\n",
       " 'Sargento Traditional Cut Shredded 4 Cheese Mexican',\n",
       " 'Sargento Traditional Cut Shredded Mozzarella Cheese',\n",
       " 'Scotch whisky',\n",
       " 'Shaoxing wine',\n",
       " 'Sicilian olives',\n",
       " 'Skippy Creamy Peanut Butter',\n",
       " 'Smart Balance Cooking Spray',\n",
       " 'Smithfield Ham',\n",
       " 'Southern Comfort Liqueur',\n",
       " 'Soy Vay Hoisin Garlic Marinade  Sauce',\n",
       " 'Soy Vay Toasted Sesame Dressing  Marinade',\n",
       " 'Soy Vay Veri Veri Teriyaki Marinade  Sauce',\n",
       " 'Spanish olives',\n",
       " 'Spanish smoked paprika',\n",
       " 'Spice Islands Bay Leaves',\n",
       " 'Spice Islands Garlic Salt',\n",
       " 'Spice Islands Ground Cumin Seed',\n",
       " 'Spice Islands Minced Garlic',\n",
       " 'Spice Islands Oregano',\n",
       " 'Spike Seasoning',\n",
       " 'Splenda Brown Sugar Blend',\n",
       " 'Spring Water',\n",
       " 'Sriracha',\n",
       " 'St Germain Liqueur',\n",
       " 'Stonefire Italian Artisan Pizza Crust',\n",
       " 'Stonefire Italian Thin Pizza Crust',\n",
       " 'Stonefire Tandoori Garlic Naan',\n",
       " 'Success White Rice',\n",
       " 'Sugar in the Raw',\n",
       " 'Swanson Chicken Broth',\n",
       " 'Swanson Vegetable Broth',\n",
       " 'Swerve Sweetener',\n",
       " 'TABASCO Chipotle Pepper Sauce',\n",
       " 'TACO BELL HOME ORIGINALS Taco Seasoning Mix',\n",
       " 'TACO BELL Thick  Chunky Mild Salsa',\n",
       " 'Tabasco Green Pepper Sauce',\n",
       " 'Tabasco Pepper Sauce',\n",
       " 'Taco Bell Taco Seasoning Mix',\n",
       " 'Taiwanese bok choy',\n",
       " 'Tamari Tamari',\n",
       " 'Tapatio Hot Sauce',\n",
       " 'Texas Pete Hot Sauce',\n",
       " 'Texas toast bread',\n",
       " 'Thai chili garlic sauce',\n",
       " 'Thai chili paste',\n",
       " 'Thai eggplants',\n",
       " 'Thai fish sauce',\n",
       " 'Thai red curry paste',\n",
       " 'Tipo 00 flour',\n",
       " 'Tokyo negi',\n",
       " 'Toulouse sausage',\n",
       " 'Truvía Baking Blend',\n",
       " 'Truvía natural sweetener',\n",
       " 'Tuaca Liqueur',\n",
       " 'Turkish bay leaves',\n",
       " 'Tuttorosso Diced Tomatoes',\n",
       " 'Tyson Crispy Chicken Strips',\n",
       " 'Uncle Bens Original Converted Brand rice',\n",
       " 'Uncle Bens Ready Rice Whole Grain Brown Rice',\n",
       " 'V8 Juice',\n",
       " 'Vadouvan curry',\n",
       " 'Vegeta Seasoning',\n",
       " 'Velveeta',\n",
       " 'Velveeta Cheese Spread',\n",
       " 'Velveeta Queso Blanco',\n",
       " 'Vietnamese coriander',\n",
       " 'White Lily Flour',\n",
       " 'Wholesome Sweeteners Organic Sugar',\n",
       " 'WishBone Italian Dressing',\n",
       " 'WishBone Light Italian Dressing',\n",
       " 'WishBone Robusto Italian Dressing',\n",
       " 'Wolf Brand Chili',\n",
       " 'Yakisoba noodles',\n",
       " 'Yoplait Greek 100 blackberry pie yogurt',\n",
       " 'Yoplait Greek 2 caramel yogurt',\n",
       " 'Yuzukosho',\n",
       " 'Zatarains Creole Seasoning',\n",
       " 'Zatarains Jambalaya Mix',\n",
       " 'abalone',\n",
       " 'abbamele',\n",
       " 'absinthe',\n",
       " 'abura age',\n",
       " 'acai juice',\n",
       " 'accent',\n",
       " 'accompaniment',\n",
       " 'achiote',\n",
       " 'achiote paste',\n",
       " 'achiote powder',\n",
       " 'acini di pepe',\n",
       " 'ackee',\n",
       " 'acorn squash',\n",
       " 'active dry yeast',\n",
       " 'adobo',\n",
       " 'adobo sauce',\n",
       " 'adobo seasoning',\n",
       " 'adobo style seasoning',\n",
       " 'adzuki beans',\n",
       " 'agar',\n",
       " 'agar agar flakes',\n",
       " 'agave nectar',\n",
       " 'agave tequila',\n",
       " 'aged Manchego cheese',\n",
       " 'aged balsamic vinegar',\n",
       " 'aged cheddar cheese',\n",
       " 'aged gouda',\n",
       " 'ahi',\n",
       " 'ahi tuna steaks',\n",
       " 'aioli',\n",
       " 'ajinomoto',\n",
       " 'ajwain',\n",
       " 'aka miso',\n",
       " 'alaskan king salmon',\n",
       " 'albacore',\n",
       " 'albacore tuna in water',\n",
       " 'alcohol',\n",
       " 'ale',\n",
       " 'aleppo',\n",
       " 'aleppo pepper',\n",
       " 'alfalfa sprouts',\n",
       " 'alfredo sauce mix',\n",
       " 'all beef hot dogs',\n",
       " 'all potato purpos',\n",
       " 'all purpose seasoning',\n",
       " 'all purpose unbleached flour',\n",
       " 'allpurpose flour',\n",
       " 'allspice',\n",
       " 'allspice berries',\n",
       " 'almond butter',\n",
       " 'almond extract',\n",
       " 'almond filling',\n",
       " 'almond flour',\n",
       " 'almond liqueur',\n",
       " 'almond meal',\n",
       " 'almond milk',\n",
       " 'almond oil',\n",
       " 'almond paste',\n",
       " 'almond syrup',\n",
       " 'almonds',\n",
       " 'aloe juice',\n",
       " 'alphabet pasta',\n",
       " 'alum',\n",
       " 'amaranth',\n",
       " 'amaretti',\n",
       " 'amaretto',\n",
       " 'amaretto liqueur',\n",
       " 'amba',\n",
       " 'amber',\n",
       " 'amber rum',\n",
       " 'amberjack fillet',\n",
       " 'amchur',\n",
       " 'america',\n",
       " 'american cheese food',\n",
       " 'american cheese slices',\n",
       " 'ammonium bicarbonate',\n",
       " 'amontillado sherry',\n",
       " 'ampalaya',\n",
       " 'anasazi beans',\n",
       " 'ancho',\n",
       " 'ancho chile pepper',\n",
       " 'ancho chili ground pepper',\n",
       " 'ancho powder',\n",
       " 'anchovies',\n",
       " 'anchovy filets',\n",
       " 'anchovy fillets',\n",
       " 'anchovy paste',\n",
       " 'and carrot green pea',\n",
       " 'and cook drain pasta ziti',\n",
       " 'and fat free half half',\n",
       " 'andouille chicken sausage',\n",
       " 'andouille sausage',\n",
       " 'andouille sausage links',\n",
       " 'andouille turkey sausages',\n",
       " 'angel food cake',\n",
       " 'angel food cake mix',\n",
       " 'angel hair',\n",
       " 'angled loofah',\n",
       " 'angus',\n",
       " 'anise',\n",
       " 'anise basil',\n",
       " 'anise extract',\n",
       " 'anise liqueur',\n",
       " 'anise oil',\n",
       " 'anise powder',\n",
       " 'anise seed',\n",
       " 'anisette',\n",
       " 'anjou pears',\n",
       " 'annatto',\n",
       " 'annatto oil',\n",
       " 'annatto powder',\n",
       " 'annatto seeds',\n",
       " 'any',\n",
       " 'aonori',\n",
       " 'apple brandy',\n",
       " 'apple butter',\n",
       " 'apple cider',\n",
       " 'apple cider vinegar',\n",
       " 'apple jelly',\n",
       " 'apple juice',\n",
       " 'apple juice concentrate',\n",
       " 'apple pie filling',\n",
       " 'apple pie spice',\n",
       " 'apple puree',\n",
       " 'apple schnapps',\n",
       " 'apple slice',\n",
       " 'apples',\n",
       " 'applesauce',\n",
       " 'applewood smoked bacon',\n",
       " 'apricot brandy',\n",
       " 'apricot halves',\n",
       " 'apricot jam',\n",
       " 'apricot jelly',\n",
       " 'apricot nectar',\n",
       " 'apricot preserves',\n",
       " 'apricots',\n",
       " 'aquavit',\n",
       " 'arak',\n",
       " 'arame',\n",
       " 'arbol chile',\n",
       " 'arborio rice',\n",
       " 'arctic char',\n",
       " 'arepa flour',\n",
       " 'arhar',\n",
       " 'arhar dal',\n",
       " 'armagnac',\n",
       " 'arrow root',\n",
       " 'arrowroot',\n",
       " 'arrowroot flour',\n",
       " 'arrowroot powder',\n",
       " 'arrowroot starch',\n",
       " 'artichok heart marin',\n",
       " 'artichoke bottoms',\n",
       " 'artichoke hearts',\n",
       " 'artichokes',\n",
       " 'artificial sweetener',\n",
       " 'artisan bread',\n",
       " 'arugula',\n",
       " 'asadero',\n",
       " 'asafetida',\n",
       " 'asafetida powder',\n",
       " 'asafoetida',\n",
       " 'asafoetida powder',\n",
       " 'asakusa nori',\n",
       " 'ascorbic acid',\n",
       " 'asiago',\n",
       " 'asian',\n",
       " 'asian barbecue sauce',\n",
       " 'asian basil',\n",
       " 'asian black bean sauce',\n",
       " 'asian chile paste',\n",
       " 'asian chili red sauc',\n",
       " 'asian dressing',\n",
       " 'asian eggplants',\n",
       " 'asian fish sauce',\n",
       " 'asian noodles',\n",
       " 'asian pear',\n",
       " 'asian rice noodles',\n",
       " 'asian wheat noodles',\n",
       " 'asparagus',\n",
       " 'asparagus bean',\n",
       " 'asparagus spears',\n",
       " 'asparagus tips',\n",
       " 'aspic',\n",
       " 'assam',\n",
       " 'assorted fresh vegetables',\n",
       " 'asti spumante',\n",
       " 'atlantic cod fillets',\n",
       " 'atta',\n",
       " 'au jus gravy',\n",
       " 'au jus gravy mix',\n",
       " 'au jus mix',\n",
       " 'avocado',\n",
       " 'avocado dressing',\n",
       " 'avocado leaves',\n",
       " 'avocado oil',\n",
       " 'awase miso',\n",
       " 'azuki bean',\n",
       " 'açai',\n",
       " 'açai powder',\n",
       " 'baby artichokes',\n",
       " 'baby arugula',\n",
       " 'baby back ribs',\n",
       " 'baby beets',\n",
       " 'baby bok choy',\n",
       " 'baby broccoli',\n",
       " 'baby carrots',\n",
       " 'baby corn',\n",
       " 'baby eggplants',\n",
       " 'baby gem lettuce',\n",
       " 'baby goat',\n",
       " 'baby greens',\n",
       " 'baby kale',\n",
       " 'baby leaf lettuce',\n",
       " 'baby lima beans',\n",
       " 'baby okra',\n",
       " 'baby portobello mushrooms',\n",
       " 'baby potatoes',\n",
       " 'baby radishes',\n",
       " 'baby spinach',\n",
       " 'baby spinach leaves',\n",
       " 'baby tatsoi',\n",
       " 'baby turnips',\n",
       " 'baby zucchini',\n",
       " 'bacardi',\n",
       " 'back bacon',\n",
       " 'back bacon rashers',\n",
       " 'back ribs',\n",
       " 'bacon',\n",
       " 'bacon bits',\n",
       " 'bacon crispcooked and crumbled',\n",
       " 'bacon drippings',\n",
       " 'bacon fat',\n",
       " 'bacon grease',\n",
       " 'bacon pieces',\n",
       " 'bacon rind',\n",
       " 'bacon salt',\n",
       " 'bacon slices',\n",
       " 'bagel chips',\n",
       " 'bagels',\n",
       " 'baguette',\n",
       " 'bai cai',\n",
       " 'baked beans',\n",
       " 'baked corn tortilla chips',\n",
       " 'baked ham',\n",
       " 'baked pita chips',\n",
       " 'baked pizza crust',\n",
       " 'baked tortilla chips',\n",
       " 'baking apples',\n",
       " 'baking chocolate',\n",
       " 'baking mix',\n",
       " 'baking potatoes',\n",
       " 'baking powder',\n",
       " 'baking soda',\n",
       " 'baking spray',\n",
       " 'baking sugar',\n",
       " 'baking yeast',\n",
       " 'balm',\n",
       " 'balm leaves',\n",
       " 'balsamic reduction',\n",
       " 'balsamic vinaigrette',\n",
       " 'balsamic vinaigrette salad dressing',\n",
       " 'balsamic vinegar',\n",
       " 'bamboo shoots',\n",
       " 'banana blossom',\n",
       " 'banana bread',\n",
       " 'banana extract',\n",
       " 'banana flower',\n",
       " 'banana leaves',\n",
       " 'banana liqueur',\n",
       " 'banana peppers',\n",
       " 'banana puree',\n",
       " 'banana squash',\n",
       " 'bananas',\n",
       " 'banger',\n",
       " 'banh hoi',\n",
       " 'banh pho rice noodles',\n",
       " 'baobab fruit powder',\n",
       " 'barbecue rub',\n",
       " 'barbecue sauce',\n",
       " 'barbecue seasoning',\n",
       " 'barbecued pork',\n",
       " 'barilla',\n",
       " 'barilla piccolini mini',\n",
       " 'barley',\n",
       " 'barley flakes',\n",
       " 'barley flour',\n",
       " 'barley grits',\n",
       " 'barley miso',\n",
       " 'barolo',\n",
       " 'barramundi fillets',\n",
       " 'bartlett pears',\n",
       " 'basa fillets',\n",
       " 'base',\n",
       " 'basil',\n",
       " 'basil dried leaves',\n",
       " 'basil leaves',\n",
       " 'basil mayonnaise',\n",
       " 'basil olive oil',\n",
       " 'basil pesto sauce',\n",
       " 'basmati',\n",
       " 'basmati rice',\n",
       " 'bass',\n",
       " 'bass fillets',\n",
       " 'baton',\n",
       " 'batter',\n",
       " 'bawang goreng',\n",
       " 'bay leaf',\n",
       " 'bay leaves',\n",
       " 'bay scallops',\n",
       " 'bbq sauce',\n",
       " 'bbq seasoning',\n",
       " 'bean curd',\n",
       " 'bean curd skins',\n",
       " 'bean curd stick',\n",
       " 'bean dip',\n",
       " 'bean paste',\n",
       " 'bean sauce',\n",
       " 'bean soup',\n",
       " 'bean soup mix',\n",
       " 'bean thread vermicelli',\n",
       " 'bean threads',\n",
       " 'beans',\n",
       " 'beansprouts',\n",
       " 'bear',\n",
       " 'beaten eggs',\n",
       " 'beau monde seasoning',\n",
       " 'beaujolais',\n",
       " 'bechamel',\n",
       " 'bee pollen',\n",
       " 'beef',\n",
       " 'beef base',\n",
       " 'beef boneless meat stew',\n",
       " 'beef bones',\n",
       " 'beef bouillon',\n",
       " 'beef bouillon granules',\n",
       " 'beef bouillon powder',\n",
       " 'beef brisket',\n",
       " 'beef broth',\n",
       " 'beef carpaccio',\n",
       " 'beef cheek',\n",
       " 'beef consomme',\n",
       " 'beef deli roast slice thinli',\n",
       " 'beef demiglace',\n",
       " 'beef drippings',\n",
       " 'beef fillet',\n",
       " 'beef for stew',\n",
       " 'beef gravy',\n",
       " 'beef heart',\n",
       " 'beef hot dogs',\n",
       " 'beef jerky',\n",
       " 'beef kidney',\n",
       " 'beef liver',\n",
       " 'beef marrow',\n",
       " 'beef rib roast',\n",
       " 'beef rib short',\n",
       " 'beef ribs',\n",
       " 'beef roast',\n",
       " 'beef round',\n",
       " 'beef rump',\n",
       " 'beef rump steaks',\n",
       " 'beef sausage',\n",
       " 'beef shank',\n",
       " 'beef shin',\n",
       " 'beef shoulder',\n",
       " 'beef shoulder roast',\n",
       " 'beef sirloin',\n",
       " 'beef smoked sausage',\n",
       " 'beef soup bones',\n",
       " 'beef steak',\n",
       " 'beef stew',\n",
       " 'beef stew meat',\n",
       " 'beef stew seasoning mix',\n",
       " 'beef stock',\n",
       " 'beef stock cubes',\n",
       " 'beef tenderloin',\n",
       " 'beef tenderloin steaks',\n",
       " 'beef tendons',\n",
       " 'beef tongue',\n",
       " 'beefsteak tomatoes',\n",
       " 'beer',\n",
       " 'beer batter',\n",
       " 'beet greens',\n",
       " 'beet juice',\n",
       " 'beets',\n",
       " 'belacan',\n",
       " 'bell pepper',\n",
       " 'beluga lentil',\n",
       " 'bengal gram',\n",
       " 'beni shoga',\n",
       " 'benne seed',\n",
       " 'bermuda onion',\n",
       " 'berries',\n",
       " 'bertolli four chees rosa sauc',\n",
       " 'bertolli organic tradit sauc',\n",
       " 'bertolli vineyard premium collect marinara with burgundi wine sauc',\n",
       " 'bertolli vodka sauc made with fresh cream',\n",
       " 'besan flour',\n",
       " 'beurre manié',\n",
       " 'bhaji',\n",
       " 'bhindi',\n",
       " 'bibb lettuce',\n",
       " 'bicarbonate of soda',\n",
       " 'biga',\n",
       " 'bigoli',\n",
       " 'bihon',\n",
       " 'bing cherries',\n",
       " 'bird chile',\n",
       " 'bird pepper',\n",
       " 'biscotti',\n",
       " 'biscuit baking mix',\n",
       " 'biscuit dough',\n",
       " 'biscuit mix',\n",
       " 'biscuits',\n",
       " 'bisquick',\n",
       " 'bitter gourd',\n",
       " 'bitter melon',\n",
       " 'bitters',\n",
       " 'bittersweet baking chocolate',\n",
       " 'bittersweet chocolate',\n",
       " 'bittersweet chocolate chips',\n",
       " 'blacan',\n",
       " 'black',\n",
       " 'black bass',\n",
       " 'black bean and corn salsa',\n",
       " 'black bean garlic sauce',\n",
       " 'black bean sauce',\n",
       " 'black bean sauce with garlic',\n",
       " 'black bean stir fry sauce',\n",
       " 'black beans',\n",
       " 'black cardamom pods',\n",
       " 'black cherries',\n",
       " 'black chicken',\n",
       " 'black cod',\n",
       " 'black cod fillets',\n",
       " 'black cumin seeds',\n",
       " 'black forest ham',\n",
       " 'black fungus',\n",
       " 'black garlic',\n",
       " 'black gram',\n",
       " 'black grapes',\n",
       " 'black lentil',\n",
       " 'black mission figs',\n",
       " 'black moss',\n",
       " 'black mushrooms',\n",
       " 'black mustard seeds',\n",
       " 'black olives',\n",
       " 'black onion seeds',\n",
       " 'black pepper',\n",
       " 'black peppercorns',\n",
       " 'black pudding',\n",
       " 'black radish',\n",
       " 'black rice',\n",
       " 'black rice vinegar',\n",
       " 'black salt',\n",
       " 'black sea bass',\n",
       " 'black sesame seeds',\n",
       " 'black tea',\n",
       " 'black tea leaves',\n",
       " 'black treacle',\n",
       " 'black truffle oil',\n",
       " 'black truffles',\n",
       " 'black trumpet mushrooms',\n",
       " 'black turtle beans',\n",
       " 'black vinegar',\n",
       " 'black walnut',\n",
       " 'blackberries',\n",
       " 'blackberry brandy',\n",
       " 'blackberry jam',\n",
       " 'blackcurrant syrup',\n",
       " 'blackening seasoning',\n",
       " 'blackeyed peas',\n",
       " 'blackpepper',\n",
       " 'blackstrap molasses',\n",
       " 'blade steak',\n",
       " 'blanched almond flour',\n",
       " 'blanched almonds',\n",
       " 'blanched hazelnuts',\n",
       " 'blanco chees queso',\n",
       " 'blanco tequila',\n",
       " 'blood',\n",
       " 'blood orange',\n",
       " 'blood orange juice',\n",
       " 'blood sausage',\n",
       " 'bloody mary mix',\n",
       " 'blue cheese',\n",
       " 'blue cheese dressing',\n",
       " 'blue corn tortilla chips',\n",
       " 'blue crabs',\n",
       " 'blue curaçao',\n",
       " 'blueberri preserv',\n",
       " ...]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = [re.sub(r'[^\\s\\w]', '', t) for t in total]\n",
    "sorted(set(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'low fat':'lowfat', ''v 8':'v8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1%', '2%', '25%', '33%', '40%', '95%', '96%'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(re.findall(r'\\([\\d\\soz\\.]+\\)', ' '.join(total)))\n",
    "set(re.findall(r'\\d+%', ' '.join(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!', '%', '&', \"'\", '(', ')', ',', '-', '.', '/', '®', '’', '€', '™'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(re.findall(r'[^\\s\\w]', ' '.join(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3589"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_ingredients = list(set(' '.join(train.ingredients).split()))\n",
    "len(set_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemmer = SnowballStemmer(\"english\")\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "word_len_sort = {}\n",
    "for w in set_ingredients:\n",
    "    w = w.lower()\n",
    "#     w = re.sub(\"[^\\w\\s]\", \"\", w)\n",
    "    if w not in stops:\n",
    "        word_len = len(w)\n",
    "        if word_len in word_len_sort:\n",
    "            word_len_sort[word_len].append(w)\n",
    "        else:\n",
    "            word_len_sort[word_len] = [w]\n",
    "\n",
    "word_len_sort.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(10',\n",
       " '(14',\n",
       " '(15',\n",
       " '1/2',\n",
       " '100',\n",
       " '25%',\n",
       " '33%',\n",
       " '40%',\n",
       " '95%',\n",
       " '96%',\n",
       " 'age',\n",
       " 'ahi',\n",
       " 'aka',\n",
       " 'ale',\n",
       " 'bag',\n",
       " 'bai',\n",
       " 'bar',\n",
       " 'bay',\n",
       " 'bbq',\n",
       " 'bee',\n",
       " 'big',\n",
       " 'bob',\n",
       " 'bok',\n",
       " 'boy',\n",
       " 'bun',\n",
       " 'cai',\n",
       " 'cod',\n",
       " 'con',\n",
       " 'cut',\n",
       " 'dal',\n",
       " 'day',\n",
       " 'dew',\n",
       " 'dip',\n",
       " 'dog',\n",
       " 'dr.',\n",
       " 'dri',\n",
       " 'dry',\n",
       " 'ear',\n",
       " 'eau',\n",
       " 'eel',\n",
       " 'egg',\n",
       " 'eye',\n",
       " 'fat',\n",
       " 'fig',\n",
       " 'fri',\n",
       " 'fry',\n",
       " 'gai',\n",
       " 'gao',\n",
       " 'gel',\n",
       " 'gem',\n",
       " 'gin',\n",
       " 'gum',\n",
       " 'guy',\n",
       " 'ham',\n",
       " 'hen',\n",
       " 'hip',\n",
       " 'hog',\n",
       " 'hoi',\n",
       " 'hot',\n",
       " 'ice',\n",
       " 'imo',\n",
       " \"it'\",\n",
       " 'jam',\n",
       " 'jif',\n",
       " 'jus',\n",
       " 'key',\n",
       " 'kha',\n",
       " 'kim',\n",
       " 'lan',\n",
       " 'lap',\n",
       " 'lb.',\n",
       " 'lea',\n",
       " 'leg',\n",
       " 'lop',\n",
       " 'low',\n",
       " 'lox',\n",
       " 'mae',\n",
       " 'mam',\n",
       " 'mex',\n",
       " 'mie',\n",
       " 'min',\n",
       " 'mix',\n",
       " 'msg',\n",
       " 'nam',\n",
       " 'new',\n",
       " 'ngo',\n",
       " 'non',\n",
       " 'nut',\n",
       " 'oat',\n",
       " 'oil',\n",
       " 'old',\n",
       " 'one',\n",
       " 'opo',\n",
       " 'pad',\n",
       " 'pak',\n",
       " 'pam',\n",
       " 'pan',\n",
       " 'pao',\n",
       " 'pat',\n",
       " 'pea',\n",
       " 'pho',\n",
       " 'pie',\n",
       " 'pig',\n",
       " 'pit',\n",
       " 'pla',\n",
       " 'pod',\n",
       " 'poi',\n",
       " 'pop',\n",
       " 'pot',\n",
       " 'puy',\n",
       " 'qua',\n",
       " 'ras',\n",
       " 'raw',\n",
       " 'red',\n",
       " 'rib',\n",
       " 'roe',\n",
       " 'rub',\n",
       " 'rum',\n",
       " 'rye',\n",
       " 'san',\n",
       " 'sea',\n",
       " 'sec',\n",
       " 'sel',\n",
       " 'siu',\n",
       " 'soi',\n",
       " 'soy',\n",
       " 'sub',\n",
       " 'sum',\n",
       " 'sun',\n",
       " 'syd',\n",
       " 'tap',\n",
       " 'tea',\n",
       " 'tex',\n",
       " 'tip',\n",
       " 'tom',\n",
       " 'ton',\n",
       " 'top',\n",
       " 'tri',\n",
       " 'tvp',\n",
       " 'ume',\n",
       " 'uni',\n",
       " 'val',\n",
       " 'vie',\n",
       " 'vin',\n",
       " 'wax',\n",
       " 'wok',\n",
       " 'yam',\n",
       " 'yum']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(word_len_sort[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['grated parmesan cheese medium shrimp Knorr® Pasta Sides™ - Alfredo baby spinach cherry tomatoes oil',\n",
       "       \"corn kernels purple onion Knorr® Fiesta Sides™ - Mexican Rice large tomato boneless sirloin steak I Can't Believe It's Not Butter!® Spread\",\n",
       "       'Knorr® Pasta Sides™ - Alfredo tomatoes fresh basil leaves provolone cheese vegetable oil boneless skinless chicken breast halves',\n",
       "       'boneless chicken skinless thigh Knorr® Pasta Sides™ - Chicken flavor cumin nonfat plain greek yogurt red bell pepper olive oil lemon juice chili powder onions',\n",
       "       \"corn kernels boneless sirloin steak I Can't Believe It's Not Butter!® Spread Knorr® Fiesta Sides™ - Mexican Rice tomatoes purple onion\",\n",
       "       'tomatoes cream cheese, soften shredded mozzarella cheese Knorr® Pasta Sides™ - Butter & Herb cut up cooked chicken frozen chopped spinach, thawed and squeezed dry'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ingredients[train.ingredients.str.contains(' - ')].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 12\n",
      "2 26\n",
      "3 169\n",
      "4 483\n",
      "5 589\n",
      "6 642\n",
      "7 551\n",
      "8 444\n",
      "9 286\n",
      "10 188\n",
      "11 85\n",
      "12 39\n",
      "13 24\n",
      "14 7\n",
      "15 5\n",
      "16 5\n",
      "18 1\n",
      "19 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in word_len_sort.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chocolatecovered',\n",
       " 'butter-margarine',\n",
       " 'hickory-flavored',\n",
       " 'vegetable-filled',\n",
       " 'chicken-flavored']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len_sort[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "def model_run(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    train_pred = model.predict(X_train) \n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    train_score = metrics.accuracy_score(y_train, train_pred)\n",
    "    test_score = metrics.accuracy_score(y_test, test_pred)\n",
    "\n",
    "    return train_score, test_score\n",
    "\n",
    "def test_on_train(model, X, y):\n",
    "    input_to_multi = []\n",
    "    start = datetime.datetime.now()\n",
    "#     for train_index, test_index in RepeatedStratifiedKFold(n_splits=5, n_repeats=4, random_state=123).split(X, y):\n",
    "    for train_index, test_index in StratifiedKFold(n_splits=5,random_state=123).split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        input_to_multi.append([model, X_train, X_test, y_train, y_test])\n",
    "\n",
    "    with multiprocessing.Pool() as p:\n",
    "        KFold_results = p.starmap(model_run, input_to_multi)\n",
    "\n",
    "\n",
    "    print(datetime.datetime.now() - start)\n",
    "    return pd.DataFrame(KFold_results,\n",
    "                        columns=['train_score', 'test_score']).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "    \n",
    "class StemmedCountVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(re.sub(\"[^\\w\\s]\", \"\", doc)) if w not in stops])\n",
    "    \n",
    "# out_index = train[train.ingredients_count > 40].index\n",
    "X = train.ingredients #.drop(out_index)\n",
    "y = train.cuisine #.drop(out_index)\n",
    "\n",
    "model = Pipeline([\n",
    "#     ('bag_of_words', CountVectorizer(tokenizer=text_process(), stop_words='english')),\n",
    "    ('bag_of_words', StemmedCountVectorizer(lowercase=True, ngram_range=(1, 2), stop_words='english')),\n",
    "\n",
    "#     ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('logreg', LogisticRegression(penalty='l2',C=10, max_iter=10000))\n",
    "#     ('logreg', OneVsRestClassifier(LogisticRegression(penalty='l2',C=10, max_iter=1000)))\n",
    "#     ('forest', RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "#                                       min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "#                                       max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                                       bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0,\n",
    "#                                       warm_start=False, class_weight=None))\n",
    "#     ('svm', OneVsRestClassifier(SVC(C=100, coef0=1)))\n",
    "#     ('boost', GradientBoostingClassifier()) #best scores\n",
    "#     ('xgb', xgb.XGBClassifier(max_depth=6, learning_rate=0.01, n_estimators=100,\n",
    "#                               objective='multi:softmax', booster='gbtree', n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-27 19:25:55.664556\n",
      "0.9837412001340932\n",
      "0.791331456154465\n",
      "2018-07-27 19:27:30.764309\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "print(datetime.datetime.now())\n",
    "# test_on_train(model, X, y)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, pd.get_dummies(y), stratify= pd.get_dummies(y))\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify= y)\n",
    "model.fit(x_train, y_train)\n",
    "train_pred = model.predict(x_train) \n",
    "test_pred = model.predict(x_test)\n",
    "\n",
    "print(metrics.accuracy_score(y_train, train_pred))\n",
    "print(metrics.accuracy_score(y_test, test_pred))\n",
    "\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39774x77936 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1369399 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = StemmedCountVectorizer(lowercase=True, ngram_range=(1, 2), stop_words='english', max_df=0.3).fit(X)\n",
    "x_trans = vec.transform(X)\n",
    "x_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<39774x77943 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1496944 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bag_of_words', StemmedCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "            dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "            lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "            ngram_range=(1, 2), norm='l2', preprocesso..._jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "importance_model = Pipeline([\n",
    "    ('bag_of_words', StemmedCountVectorizer(lowercase=True, ngram_range=(1, 2), stop_words='english')),\n",
    "    ('forest', RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                                      min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "                                      bootstrap=True, oob_score=False, n_jobs=-1,\n",
    "                                      warm_start=False, class_weight=None))\n",
    "\n",
    "])\n",
    "importance_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = importance_model.steps[-1][-1].feature_importances_\n",
    "columns = importance_model.steps[0][-1].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>f_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77942</th>\n",
       "      <td>8.670534e-07</td>\n",
       "      <td>épices shallot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77941</th>\n",
       "      <td>2.498326e-06</td>\n",
       "      <td>épices salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77940</th>\n",
       "      <td>7.809187e-07</td>\n",
       "      <td>épices raisin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77939</th>\n",
       "      <td>1.324320e-06</td>\n",
       "      <td>épices larg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77938</th>\n",
       "      <td>4.037788e-06</td>\n",
       "      <td>épice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77937</th>\n",
       "      <td>5.574334e-08</td>\n",
       "      <td>zucchini zucchini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77936</th>\n",
       "      <td>7.452059e-07</td>\n",
       "      <td>zucchini yukon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77935</th>\n",
       "      <td>1.330238e-05</td>\n",
       "      <td>zucchini yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77934</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>zucchini worcestershir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77933</th>\n",
       "      <td>5.834038e-07</td>\n",
       "      <td>zucchini wishbon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77932</th>\n",
       "      <td>2.820752e-06</td>\n",
       "      <td>zucchini win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77931</th>\n",
       "      <td>1.028032e-08</td>\n",
       "      <td>zucchini whit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77930</th>\n",
       "      <td>4.656342e-06</td>\n",
       "      <td>zucchini whip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77929</th>\n",
       "      <td>1.376040e-06</td>\n",
       "      <td>zucchini wheat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77928</th>\n",
       "      <td>1.379264e-06</td>\n",
       "      <td>zucchini waxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77927</th>\n",
       "      <td>4.540465e-07</td>\n",
       "      <td>zucchini wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77926</th>\n",
       "      <td>1.594498e-07</td>\n",
       "      <td>zucchini vietnames</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77925</th>\n",
       "      <td>1.127261e-05</td>\n",
       "      <td>zucchini veget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77924</th>\n",
       "      <td>6.128703e-07</td>\n",
       "      <td>zucchini unsalt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77923</th>\n",
       "      <td>1.049375e-06</td>\n",
       "      <td>zucchini turnip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77922</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>zucchini tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77921</th>\n",
       "      <td>5.172036e-08</td>\n",
       "      <td>zucchini toast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77920</th>\n",
       "      <td>5.914910e-06</td>\n",
       "      <td>zucchini thym</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77919</th>\n",
       "      <td>2.542668e-07</td>\n",
       "      <td>zucchini thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77918</th>\n",
       "      <td>8.633616e-07</td>\n",
       "      <td>zucchini tamari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77917</th>\n",
       "      <td>8.750470e-09</td>\n",
       "      <td>zucchini taco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77916</th>\n",
       "      <td>9.834962e-09</td>\n",
       "      <td>zucchini sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77915</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>zucchini summ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77914</th>\n",
       "      <td>1.208237e-06</td>\n",
       "      <td>zucchini sriracha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77913</th>\n",
       "      <td>1.202417e-06</td>\n",
       "      <td>zucchini squid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77912</th>\n",
       "      <td>2.029090e-06</td>\n",
       "      <td>zucchini spic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77911</th>\n",
       "      <td>7.460674e-07</td>\n",
       "      <td>zucchini spaghettini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77910</th>\n",
       "      <td>2.556402e-06</td>\n",
       "      <td>zucchini soybean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77909</th>\n",
       "      <td>7.681641e-07</td>\n",
       "      <td>zucchini sour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77908</th>\n",
       "      <td>7.058637e-07</td>\n",
       "      <td>zucchini sorrel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77907</th>\n",
       "      <td>4.192542e-06</td>\n",
       "      <td>zucchini sl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77906</th>\n",
       "      <td>2.012938e-06</td>\n",
       "      <td>zucchini skinless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77905</th>\n",
       "      <td>1.377476e-06</td>\n",
       "      <td>zucchini shrimp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77904</th>\n",
       "      <td>4.311650e-06</td>\n",
       "      <td>zucchini shred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77903</th>\n",
       "      <td>3.047161e-06</td>\n",
       "      <td>zucchini sherri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77902</th>\n",
       "      <td>2.032220e-06</td>\n",
       "      <td>zucchini shallot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77901</th>\n",
       "      <td>4.030859e-06</td>\n",
       "      <td>zucchini sesam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77900</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>zucchini season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77899</th>\n",
       "      <td>5.486239e-06</td>\n",
       "      <td>zucchini sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77898</th>\n",
       "      <td>3.967085e-06</td>\n",
       "      <td>zucchini scallion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77897</th>\n",
       "      <td>1.569557e-07</td>\n",
       "      <td>zucchini sausag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77896</th>\n",
       "      <td>1.354052e-06</td>\n",
       "      <td>zucchini sauc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77895</th>\n",
       "      <td>3.427072e-05</td>\n",
       "      <td>zucchini salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77894</th>\n",
       "      <td>8.697432e-07</td>\n",
       "      <td>zucchini salsa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77893</th>\n",
       "      <td>3.278528e-07</td>\n",
       "      <td>zucchini salmon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores                  f_name\n",
       "77942  8.670534e-07          épices shallot\n",
       "77941  2.498326e-06             épices salt\n",
       "77940  7.809187e-07           épices raisin\n",
       "77939  1.324320e-06             épices larg\n",
       "77938  4.037788e-06                   épice\n",
       "77937  5.574334e-08       zucchini zucchini\n",
       "77936  7.452059e-07          zucchini yukon\n",
       "77935  1.330238e-05         zucchini yellow\n",
       "77934  0.000000e+00  zucchini worcestershir\n",
       "77933  5.834038e-07        zucchini wishbon\n",
       "77932  2.820752e-06            zucchini win\n",
       "77931  1.028032e-08           zucchini whit\n",
       "77930  4.656342e-06           zucchini whip\n",
       "77929  1.376040e-06          zucchini wheat\n",
       "77928  1.379264e-06           zucchini waxi\n",
       "77927  4.540465e-07            zucchini wat\n",
       "77926  1.594498e-07      zucchini vietnames\n",
       "77925  1.127261e-05          zucchini veget\n",
       "77924  6.128703e-07         zucchini unsalt\n",
       "77923  1.049375e-06         zucchini turnip\n",
       "77922  0.000000e+00         zucchini tomato\n",
       "77921  5.172036e-08          zucchini toast\n",
       "77920  5.914910e-06           zucchini thym\n",
       "77919  2.542668e-07           zucchini thai\n",
       "77918  8.633616e-07         zucchini tamari\n",
       "77917  8.750470e-09           zucchini taco\n",
       "77916  9.834962e-09          zucchini sweet\n",
       "77915  0.000000e+00           zucchini summ\n",
       "77914  1.208237e-06       zucchini sriracha\n",
       "77913  1.202417e-06          zucchini squid\n",
       "77912  2.029090e-06           zucchini spic\n",
       "77911  7.460674e-07    zucchini spaghettini\n",
       "77910  2.556402e-06        zucchini soybean\n",
       "77909  7.681641e-07           zucchini sour\n",
       "77908  7.058637e-07         zucchini sorrel\n",
       "77907  4.192542e-06             zucchini sl\n",
       "77906  2.012938e-06       zucchini skinless\n",
       "77905  1.377476e-06         zucchini shrimp\n",
       "77904  4.311650e-06          zucchini shred\n",
       "77903  3.047161e-06         zucchini sherri\n",
       "77902  2.032220e-06        zucchini shallot\n",
       "77901  4.030859e-06          zucchini sesam\n",
       "77900  0.000000e+00         zucchini season\n",
       "77899  5.486239e-06            zucchini sea\n",
       "77898  3.967085e-06       zucchini scallion\n",
       "77897  1.569557e-07         zucchini sausag\n",
       "77896  1.354052e-06           zucchini sauc\n",
       "77895  3.427072e-05           zucchini salt\n",
       "77894  8.697432e-07          zucchini salsa\n",
       "77893  3.278528e-07         zucchini salmon"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = pd.DataFrame(list(zip(score, columns)), columns=['scores', 'f_name']).sort_values('f_name', ascending=False)\n",
    "importance.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00013847669723380642"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(score))[-1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count the number of words in each ingridiant for the n_grams //Done\n",
    "# Try to find outliers in num of ingridiants\n",
    "# try plying with the CountVectorizer/TFIDF params to drop outlier ingridiants\n",
    "# drop corr features\n",
    "# Param CV search\n",
    "### Toknizer + stemmer //Done\n",
    "# Word2Vec: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpora': {'20-newsgroups': {'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
       "   'description': 'The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.',\n",
       "   'fields': {'data': '',\n",
       "    'id': 'original id inferred from folder name',\n",
       "    'set': \"marker of original split (possible values 'train' and 'test')\",\n",
       "    'topic': 'name of topic (20 variant of possible values)'},\n",
       "   'file_name': '20-newsgroups.gz',\n",
       "   'file_size': 14483581,\n",
       "   'license': 'not found',\n",
       "   'num_records': 18846,\n",
       "   'parts': 1,\n",
       "   'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
       "   'record_format': 'dict'},\n",
       "  '__testing_matrix-synopsis': {'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
       "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
       "   'file_name': '__testing_matrix-synopsis.gz',\n",
       "   'parts': 1,\n",
       "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
       "  '__testing_multipart-matrix-synopsis': {'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
       "   'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
       "   'checksum-2': '966db9d274d125beaac7987202076cba',\n",
       "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
       "   'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
       "   'parts': 3,\n",
       "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
       "  'fake-news': {'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
       "   'description': \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
       "   'fields': {'author': 'author of story',\n",
       "    'comments': 'number of Facebook comments',\n",
       "    'country': 'data from webhose.io',\n",
       "    'crawled': 'date the story was archived',\n",
       "    'domain_rank': 'data from webhose.io',\n",
       "    'language': 'data from webhose.io',\n",
       "    'likes': 'number of Facebook likes',\n",
       "    'main_img_url': 'image from story',\n",
       "    'ord_in_thread': '',\n",
       "    'participants_count': 'number of participants',\n",
       "    'published': 'date published',\n",
       "    'replies_count': 'number of replies',\n",
       "    'shares': 'number of Facebook shares',\n",
       "    'site_url': 'site URL from BS detector',\n",
       "    'spam_score': 'data from webhose.io',\n",
       "    'text': 'text of story',\n",
       "    'thread_title': '',\n",
       "    'title': 'title of story',\n",
       "    'type': 'type of website (label from BS detector)',\n",
       "    'uuid': 'unique identifier'},\n",
       "   'file_name': 'fake-news.gz',\n",
       "   'file_size': 20102776,\n",
       "   'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
       "   'num_records': 12999,\n",
       "   'parts': 1,\n",
       "   'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
       "   'record_format': 'dict'},\n",
       "  'patent-2017': {'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
       "   'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
       "   'description': \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
       "   'file_name': 'patent-2017.gz',\n",
       "   'file_size': 3087262469,\n",
       "   'license': 'not found',\n",
       "   'num_records': 353197,\n",
       "   'parts': 2,\n",
       "   'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
       "   'record_format': 'dict'},\n",
       "  'quora-duplicate-questions': {'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
       "   'description': 'Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.',\n",
       "   'fields': {'id': 'the id of a training set question pair',\n",
       "    'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise',\n",
       "    'qid1': 'unique ids of each question',\n",
       "    'qid2': 'unique ids of each question',\n",
       "    'question1': 'the full text of each question',\n",
       "    'question2': 'the full text of each question'},\n",
       "   'file_name': 'quora-duplicate-questions.gz',\n",
       "   'file_size': 21684784,\n",
       "   'license': 'probably https://www.quora.com/about/tos',\n",
       "   'num_records': 404290,\n",
       "   'parts': 1,\n",
       "   'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
       "   'record_format': 'dict'},\n",
       "  'semeval-2016-2017-task3-subtaskA-unannotated': {'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
       "   'description': 'SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.',\n",
       "   'fields': {'RelComments': [{'RELC_DATE': 'date of posting',\n",
       "      'RELC_ID': 'comment identifier',\n",
       "      'RELC_USERID': 'identifier of the user posting the comment',\n",
       "      'RELC_USERNAME': 'name of the user posting the comment',\n",
       "      'RelCText': 'text of answer'}],\n",
       "    'RelQuestion': {'RELQ_CATEGORY': 'question category, according to the Qatar Living taxonomy',\n",
       "     'RELQ_DATE': 'date of posting',\n",
       "     'RELQ_ID': 'question indentifier',\n",
       "     'RELQ_USERID': 'identifier of the user asking the question',\n",
       "     'RELQ_USERNAME': 'name of the user asking the question',\n",
       "     'RelQBody': 'body of question',\n",
       "     'RelQSubject': 'subject of question'},\n",
       "    'THREAD_SEQUENCE': ''},\n",
       "   'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
       "   'file_size': 234373151,\n",
       "   'license': 'These datasets are free for general research use.',\n",
       "   'num_records': 189941,\n",
       "   'parts': 1,\n",
       "   'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
       "    'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
       "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
       "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
       "   'record_format': 'dict'},\n",
       "  'semeval-2016-2017-task3-subtaskBC': {'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
       "   'description': 'SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section “Papers” of https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
       "   'fields': {'2016-dev': ['...'],\n",
       "    '2016-test': ['...'],\n",
       "    '2016-train': ['...'],\n",
       "    '2017-test': ['...']},\n",
       "   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
       "   'file_size': 6344358,\n",
       "   'license': 'All files released for the task are free for general research use',\n",
       "   'num_records': -1,\n",
       "   'parts': 1,\n",
       "   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
       "    'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
       "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
       "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
       "   'record_format': 'dict'},\n",
       "  'text8': {'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
       "   'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
       "   'file_name': 'text8.gz',\n",
       "   'file_size': 33182058,\n",
       "   'license': 'not found',\n",
       "   'num_records': 1701,\n",
       "   'parts': 1,\n",
       "   'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
       "   'record_format': 'list of str (tokens)'},\n",
       "  'wiki-english-20171001': {'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
       "   'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
       "   'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
       "   'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
       "   'description': 'Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`',\n",
       "   'fields': {'section_texts': 'list of body of sections',\n",
       "    'section_titles': 'list of titles of sections',\n",
       "    'title': 'Title of wiki article'},\n",
       "   'file_name': 'wiki-english-20171001.gz',\n",
       "   'file_size': 6516051717,\n",
       "   'license': 'https://dumps.wikimedia.org/legal.html',\n",
       "   'num_records': 4924894,\n",
       "   'parts': 4,\n",
       "   'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
       "   'record_format': 'dict'}},\n",
       " 'models': {'__testing_word2vec-matrix-synopsis': {'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
       "   'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
       "   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
       "   'parameters': {'dimensions': 50},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
       "   'read_more': []},\n",
       "  'conceptnet-numberbatch-17-06-300': {'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
       "   'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
       "   'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
       "   'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
       "   'file_size': 1225497562,\n",
       "   'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
       "   'num_records': 1917247,\n",
       "   'parameters': {'dimension': 300},\n",
       "   'parts': 1,\n",
       "   'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
       "    'https://github.com/commonsense/conceptnet-numberbatch',\n",
       "    'http://conceptnet.io/'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py'},\n",
       "  'fasttext-wiki-news-subwords-300': {'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
       "   'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
       "   'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
       "   'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
       "   'file_size': 1005007116,\n",
       "   'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
       "   'num_records': 999999,\n",
       "   'parameters': {'dimension': 300},\n",
       "   'parts': 1,\n",
       "   'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
       "    'https://arxiv.org/abs/1712.09405',\n",
       "    'https://arxiv.org/abs/1607.01759'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py'},\n",
       "  'glove-twitter-100': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "   'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
       "   'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
       "   'file_name': 'glove-twitter-100.gz',\n",
       "   'file_size': 405932991,\n",
       "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "   'num_records': 1193514,\n",
       "   'parameters': {'dimension': 100},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
       "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py'},\n",
       "  'glove-twitter-200': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "   'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
       "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "   'file_name': 'glove-twitter-200.gz',\n",
       "   'file_size': 795373100,\n",
       "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "   'num_records': 1193514,\n",
       "   'parameters': {'dimension': 200},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
       "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py'},\n",
       "  'glove-twitter-25': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "   'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
       "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "   'file_name': 'glove-twitter-25.gz',\n",
       "   'file_size': 109885004,\n",
       "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "   'num_records': 1193514,\n",
       "   'parameters': {'dimension': 25},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
       "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py'},\n",
       "  'glove-twitter-50': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "   'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
       "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
       "   'file_name': 'glove-twitter-50.gz',\n",
       "   'file_size': 209216938,\n",
       "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "   'num_records': 1193514,\n",
       "   'parameters': {'dimension': 50},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
       "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py'},\n",
       "  'glove-wiki-gigaword-100': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "   'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
       "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "   'file_name': 'glove-wiki-gigaword-100.gz',\n",
       "   'file_size': 134300434,\n",
       "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "   'num_records': 400000,\n",
       "   'parameters': {'dimension': 100},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
       "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py'},\n",
       "  'glove-wiki-gigaword-200': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "   'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
       "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "   'file_name': 'glove-wiki-gigaword-200.gz',\n",
       "   'file_size': 264336934,\n",
       "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "   'num_records': 400000,\n",
       "   'parameters': {'dimension': 200},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
       "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py'},\n",
       "  'glove-wiki-gigaword-300': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "   'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
       "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "   'file_name': 'glove-wiki-gigaword-300.gz',\n",
       "   'file_size': 394362229,\n",
       "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "   'num_records': 400000,\n",
       "   'parameters': {'dimension': 300},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
       "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py'},\n",
       "  'glove-wiki-gigaword-50': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "   'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "   'file_name': 'glove-wiki-gigaword-50.gz',\n",
       "   'file_size': 69182535,\n",
       "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "   'num_records': 400000,\n",
       "   'parameters': {'dimension': 50},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'},\n",
       "  'word2vec-google-news-300': {'base_dataset': 'Google News (about 100 billion words)',\n",
       "   'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
       "   'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
       "   'file_name': 'word2vec-google-news-300.gz',\n",
       "   'file_size': 1743563840,\n",
       "   'license': 'not found',\n",
       "   'num_records': 3000000,\n",
       "   'parameters': {'dimension': 300},\n",
       "   'parts': 1,\n",
       "   'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
       "    'https://arxiv.org/abs/1301.3781',\n",
       "    'https://arxiv.org/abs/1310.4546',\n",
       "    'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'},\n",
       "  'word2vec-ruscorpora-300': {'base_dataset': 'Russian National Corpus (about 250M words)',\n",
       "   'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
       "   'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
       "   'file_name': 'word2vec-ruscorpora-300.gz',\n",
       "   'file_size': 208427381,\n",
       "   'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
       "   'num_records': 184973,\n",
       "   'parameters': {'dimension': 300, 'window_size': 10},\n",
       "   'parts': 1,\n",
       "   'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
       "   'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
       "    'http://rusvectores.org/en/',\n",
       "    'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
       "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "v = api.info()\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__testing_word2vec-matrix-synopsis\": {\n",
      "        \"checksum\": \"534dcb8b56a360977a269b7bfc62d124\",\n",
      "        \"description\": \"[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.\",\n",
      "        \"file_name\": \"__testing_word2vec-matrix-synopsis.gz\",\n",
      "        \"parameters\": {\n",
      "            \"dimensions\": 50\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.\",\n",
      "        \"read_more\": []\n",
      "    },\n",
      "    \"conceptnet-numberbatch-17-06-300\": {\n",
      "        \"base_dataset\": \"ConceptNet, word2vec, GloVe, and OpenSubtitles 2016\",\n",
      "        \"checksum\": \"fd642d457adcd0ea94da0cd21b150847\",\n",
      "        \"description\": \"ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.\",\n",
      "        \"file_name\": \"conceptnet-numberbatch-17-06-300.gz\",\n",
      "        \"file_size\": 1225497562,\n",
      "        \"license\": \"https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt\",\n",
      "        \"num_records\": 1917247,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 300\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"read_more\": [\n",
      "            \"http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972\",\n",
      "            \"https://github.com/commonsense/conceptnet-numberbatch\",\n",
      "            \"http://conceptnet.io/\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py\"\n",
      "    },\n",
      "    \"fasttext-wiki-news-subwords-300\": {\n",
      "        \"base_dataset\": \"Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)\",\n",
      "        \"checksum\": \"de2bb3a20c46ce65c9c131e1ad9a77af\",\n",
      "        \"description\": \"1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).\",\n",
      "        \"file_name\": \"fasttext-wiki-news-subwords-300.gz\",\n",
      "        \"file_size\": 1005007116,\n",
      "        \"license\": \"https://creativecommons.org/licenses/by-sa/3.0/\",\n",
      "        \"num_records\": 999999,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 300\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"read_more\": [\n",
      "            \"https://fasttext.cc/docs/en/english-vectors.html\",\n",
      "            \"https://arxiv.org/abs/1712.09405\",\n",
      "            \"https://arxiv.org/abs/1607.01759\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py\"\n",
      "    },\n",
      "    \"glove-twitter-100\": {\n",
      "        \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "        \"checksum\": \"b04f7bed38756d64cf55b58ce7e97b15\",\n",
      "        \"description\": \"Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\",\n",
      "        \"file_name\": \"glove-twitter-100.gz\",\n",
      "        \"file_size\": 405932991,\n",
      "        \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "        \"num_records\": 1193514,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 100\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.\",\n",
      "        \"read_more\": [\n",
      "            \"https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py\"\n",
      "    },\n",
      "    \"glove-twitter-200\": {\n",
      "        \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "        \"checksum\": \"e52e8392d1860b95d5308a525817d8f9\",\n",
      "        \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "        \"file_name\": \"glove-twitter-200.gz\",\n",
      "        \"file_size\": 795373100,\n",
      "        \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "        \"num_records\": 1193514,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 200\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.\",\n",
      "        \"read_more\": [\n",
      "            \"https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py\"\n",
      "    },\n",
      "    \"glove-twitter-25\": {\n",
      "        \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "        \"checksum\": \"50db0211d7e7a2dcd362c6b774762793\",\n",
      "        \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "        \"file_name\": \"glove-twitter-25.gz\",\n",
      "        \"file_size\": 109885004,\n",
      "        \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "        \"num_records\": 1193514,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 25\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.\",\n",
      "        \"read_more\": [\n",
      "            \"https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py\"\n",
      "    },\n",
      "    \"glove-twitter-50\": {\n",
      "        \"base_dataset\": \"Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)\",\n",
      "        \"checksum\": \"c168f18641f8c8a00fe30984c4799b2b\",\n",
      "        \"description\": \"Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)\",\n",
      "        \"file_name\": \"glove-twitter-50.gz\",\n",
      "        \"file_size\": 209216938,\n",
      "        \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "        \"num_records\": 1193514,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 50\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.\",\n",
      "        \"read_more\": [\n",
      "            \"https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py\"\n",
      "    },\n",
      "    \"glove-wiki-gigaword-100\": {\n",
      "        \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "        \"checksum\": \"40ec481866001177b8cd4cb0df92924f\",\n",
      "        \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "        \"file_name\": \"glove-wiki-gigaword-100.gz\",\n",
      "        \"file_size\": 134300434,\n",
      "        \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "        \"num_records\": 400000,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 100\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.\",\n",
      "        \"read_more\": [\n",
      "            \"https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py\"\n",
      "    },\n",
      "    \"glove-wiki-gigaword-200\": {\n",
      "        \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "        \"checksum\": \"59652db361b7a87ee73834a6c391dfc1\",\n",
      "        \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "        \"file_name\": \"glove-wiki-gigaword-200.gz\",\n",
      "        \"file_size\": 264336934,\n",
      "        \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "        \"num_records\": 400000,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 200\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.\",\n",
      "        \"read_more\": [\n",
      "            \"https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py\"\n",
      "    },\n",
      "    \"glove-wiki-gigaword-300\": {\n",
      "        \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "        \"checksum\": \"29e9329ac2241937d55b852e8284e89b\",\n",
      "        \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "        \"file_name\": \"glove-wiki-gigaword-300.gz\",\n",
      "        \"file_size\": 394362229,\n",
      "        \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "        \"num_records\": 400000,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 300\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.\",\n",
      "        \"read_more\": [\n",
      "            \"https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py\"\n",
      "    },\n",
      "    \"glove-wiki-gigaword-50\": {\n",
      "        \"base_dataset\": \"Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)\",\n",
      "        \"checksum\": \"c289bc5d7f2f02c6dc9f2f9b67641813\",\n",
      "        \"description\": \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).\",\n",
      "        \"file_name\": \"glove-wiki-gigaword-50.gz\",\n",
      "        \"file_size\": 69182535,\n",
      "        \"license\": \"http://opendatacommons.org/licenses/pddl/\",\n",
      "        \"num_records\": 400000,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 50\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.\",\n",
      "        \"read_more\": [\n",
      "            \"https://nlp.stanford.edu/projects/glove/\",\n",
      "            \"https://nlp.stanford.edu/pubs/glove.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py\"\n",
      "    },\n",
      "    \"word2vec-google-news-300\": {\n",
      "        \"base_dataset\": \"Google News (about 100 billion words)\",\n",
      "        \"checksum\": \"a5e5354d40acb95f9ec66d5977d140ef\",\n",
      "        \"description\": \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
      "        \"file_name\": \"word2vec-google-news-300.gz\",\n",
      "        \"file_size\": 1743563840,\n",
      "        \"license\": \"not found\",\n",
      "        \"num_records\": 3000000,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 300\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"read_more\": [\n",
      "            \"https://code.google.com/archive/p/word2vec/\",\n",
      "            \"https://arxiv.org/abs/1301.3781\",\n",
      "            \"https://arxiv.org/abs/1310.4546\",\n",
      "            \"https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py\"\n",
      "    },\n",
      "    \"word2vec-ruscorpora-300\": {\n",
      "        \"base_dataset\": \"Russian National Corpus (about 250M words)\",\n",
      "        \"checksum\": \"9bdebdc8ae6d17d20839dd9b5af10bc4\",\n",
      "        \"description\": \"Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.\",\n",
      "        \"file_name\": \"word2vec-ruscorpora-300.gz\",\n",
      "        \"file_size\": 208427381,\n",
      "        \"license\": \"https://creativecommons.org/licenses/by/4.0/deed.en\",\n",
      "        \"num_records\": 184973,\n",
      "        \"parameters\": {\n",
      "            \"dimension\": 300,\n",
      "            \"window_size\": 10\n",
      "        },\n",
      "        \"parts\": 1,\n",
      "        \"preprocessing\": \"The corpus was lemmatized and tagged with Universal PoS\",\n",
      "        \"read_more\": [\n",
      "            \"https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models\",\n",
      "            \"http://rusvectores.org/en/\",\n",
      "            \"https://github.com/RaRe-Technologies/gensim-data/issues/3\"\n",
      "        ],\n",
      "        \"reader_code\": \"https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "print(json.dumps(v['models'], indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================--------------------------] 49.8% 582.0/1168.7MB downloaded"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2cfae20a2a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conceptnet-numberbatch-17-06-300'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{fname}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m    935\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                   self.__class__)\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \"\"\"\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, model_path = api.load('conceptnet-numberbatch-17-06-300', return_path=True)\n",
    "model.most_similar(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-20994c22a455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "model_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
